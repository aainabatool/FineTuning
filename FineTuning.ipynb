{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOVpuqs2/RHUlD8+zQE+hWn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aainabatool/FineTuning/blob/main/FineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FineTuning DistilBert Uncased for Emotions DataSet**"
      ],
      "metadata": {
        "id": "SoAY6R13QKEI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "W22acPiUpLXo"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers evaluate accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uM2AQOioQTi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import evaluate\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "mRgw68yqpdrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load DataSet**\n"
      ],
      "metadata": {
        "id": "F3OYY593pkVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"emotion\")\n",
        "dataset\n"
      ],
      "metadata": {
        "id": "9tK4Vctnpovc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "9IslMghSpyEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "encoded_dataset = dataset.map(tokenize, batched=True)\n"
      ],
      "metadata": {
        "id": "pctZut5ap0Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dataset = encoded_dataset.rename_column(\"label\", \"labels\")   #Renames label → labels (Trainer expects labels).\n",
        "encoded_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])  #Converts dataset to PyTorch tensors so Trainer can use them."
      ],
      "metadata": {
        "id": "O0gv7hM2p8vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Model**"
      ],
      "metadata": {
        "id": "BEi7p3VhqABP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "“We add a classification head because DistilBERT by itself only produces embeddings. The head converts those embeddings into class probabilities (for our 6 emotions). Without it, the model can’t do classification.”"
      ],
      "metadata": {
        "id": "Y1qa_Df_0Vzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "“The input sentence is first tokenized. DistilBERT processes tokens and outputs embeddings. The special [CLS] token represents the whole sentence. We pass that through a classification head (linear + softmax), which gives probabilities for each emotion class. That’s how the model predicts whether the text shows anger, joy, sadness, etc.”"
      ],
      "metadata": {
        "id": "RwPrDUSB0-Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = dataset[\"train\"].features[\"label\"].num_classes\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    checkpoint, num_labels=num_labels\n",
        ")\n"
      ],
      "metadata": {
        "id": "FYuz4X24p-iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model outputs logits → converted to probabilities → used for classification."
      ],
      "metadata": {
        "id": "xBdYQhYt10Yb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metrics**"
      ],
      "metadata": {
        "id": "GMfzMUfOqHae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "logits = raw model outputs before softmax\n",
        "\n",
        "argmax → picks the class with highest score.\n",
        "\n",
        "Accuracy → measures overall correctness.\n",
        "\n",
        "F1-macro → averages performance across all classes equally (important for unbalanced datasets)."
      ],
      "metadata": {
        "id": "xdoRhAcB2ORk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy.compute(predictions=predictions, references=labels)\n",
        "    f1_macro = f1.compute(predictions=predictions, references=labels, average=\"macro\")\n",
        "    return {\"accuracy\": acc[\"accuracy\"], \"f1_macro\": f1_macro[\"f1\"]}\n"
      ],
      "metadata": {
        "id": "uYfR-QjYqHwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "“No, logits are not the predictions. They are the raw, unnormalized scores output by the model. We apply softmax to convert them into probabilities, and then take the argmax to get the actual predicted class.”"
      ],
      "metadata": {
        "id": "pafSQSE74LBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Arguments**"
      ],
      "metadata": {
        "id": "vCFfR9MmqNBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,        #prevents overfitting.\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ],
      "metadata": {
        "id": "NitQehmVqQDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trainer**"
      ],
      "metadata": {
        "id": "3RK4_kM4qtue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ],
      "metadata": {
        "id": "113p_3EXqq9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train**"
      ],
      "metadata": {
        "id": "I3jx2P81q0f2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7065d1e896eff136eefcd1180b500eb98346a857\n"
      ],
      "metadata": {
        "id": "29PmB2PcsUz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "vpknKF5Oq4cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**"
      ],
      "metadata": {
        "id": "e_Jpc7CPq8JP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_finetuned = trainer.evaluate(encoded_dataset[\"test\"])\n",
        "print(results_finetuned)\n"
      ],
      "metadata": {
        "id": "3nC0HlOnq-oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Compare with Base (Pretrained) Model**"
      ],
      "metadata": {
        "id": "IFYPD_YFtzBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Zero-shot (no fine-tuning)\n",
        "clf = pipeline(\"text-classification\", model=checkpoint, tokenizer=tokenizer)\n",
        "sample = dataset[\"test\"][0][\"text\"]\n",
        "print(\"Sample text:\", sample)\n",
        "print(\"Base Model Prediction:\", clf(sample))\n"
      ],
      "metadata": {
        "id": "PaZrMfvmttg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_finetuned = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
        "print(\"Fine-Tuned Prediction:\", clf_finetuned(sample))\n"
      ],
      "metadata": {
        "id": "69NOnboRuJAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix**"
      ],
      "metadata": {
        "id": "AnyQsUpZuWYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "preds = trainer.predict(encoded_dataset[\"test\"])\n",
        "y_true = preds.label_ids\n",
        "y_pred = np.argmax(preds.predictions, axis=1)\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(y_true, y_pred, display_labels=dataset[\"train\"].features[\"label\"].names)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "70vbVBIduQsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ows = True labels (actual emotion in dataset).\n",
        "\n",
        "Columns = Predicted labels (what model guessed).\n",
        "\n",
        "Diagonal values = correct predictions (good).\n",
        "\n",
        "Off-diagonal values = misclassifications (errors)."
      ],
      "metadata": {
        "id": "c3TVJHmb6U00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load your fine-tuned model + tokenizer\n",
        "emotion_clf = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"./results/checkpoint-best\",   # path where Trainer saved your best model\n",
        "    tokenizer=\"distilbert-base-uncased\",\n",
        "    return_all_scores=True   # so we can see probabilities for all classes\n",
        ")\n",
        "\n",
        "# --- Test cases ---\n",
        "texts = [\n",
        "    \"I am so happy to see you again!\",        # joy\n",
        "    \"I feel really lonely and sad today.\",    # sadness\n",
        "    \"I love spending time with my family.\",   # love\n",
        "    \"This situation makes me so angry!\",      # anger\n",
        "    \"I was scared walking home at night.\",    # fear\n",
        "    \"Wow, I didn’t expect that surprise!\",    # surprise\n",
        "]\n",
        "\n",
        "# Run predictions\n",
        "for t in texts:\n",
        "    result = emotion_clf(t)\n",
        "    print(f\"\\nInput: {t}\")\n",
        "    for r in result[0]:\n",
        "        print(f\"{r['label']}: {r['score']:.3f}\")"
      ],
      "metadata": {
        "id": "YgTj4y6b7aZ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}